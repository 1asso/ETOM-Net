Epoch 1, Learning rate 0.0001
====================
{'lr': 0.0001, 'betas': (0.9, 0.999)}
====================
Training epoch # 1, totaling mini batches 150


 | Epoch (train): [1][30/150]
Scale 0: mask: 0.08659752135475476, Scale 0: rho: 0.13583369925618172, Scale 0: flow: 0.008871148511146506, Scale 0: rec: 0.0008922558918129653, Scale 1: mask: 0.17750955671072005, Scale 1: rho: 0.318694506585598, Scale 1: flow: 0.036797858153780304, Scale 1: rec: 0.0018963640904985368, Scale 2: mask: 0.35184048215548197, Scale 2: rho: 0.6198401560386022, Scale 2: flow: 0.15246595740318297, Scale 2: rec: 0.004000826901756227, Scale 3: mask: 0.7212076445420583, Scale 3: rho: 1.1725537409385045, Scale 3: flow: 1.3768385807673136, Scale 3: rec: 0.1719620962937673,  [Total Loss: 5.33780239559516]


 | Epoch (train): [1][60/150]
Scale 0: mask: 0.0872525009016196, Scale 0: rho: 0.15743690803647042, Scale 0: flow: 0.00932352280554672, Scale 0: rec: 0.000960022498232623, Scale 1: mask: 0.17256478269894918, Scale 1: rho: 0.2494708185394605, Scale 1: flow: 0.0386897649616003, Scale 1: rec: 0.0019845325616188346, Scale 2: mask: 0.35250916381676994, Scale 2: rho: 0.5272417709231376, Scale 2: flow: 0.16023690849542618, Scale 2: rec: 0.004054710594937205, Scale 3: mask: 0.6819761137167613, Scale 3: rho: 1.1241934011379877, Scale 3: flow: 1.4423638780911763, Scale 3: rec: 0.18202490756909054,  [Total Loss: 5.192283707348785]


 | Epoch (train): [1][90/150]
Scale 0: mask: 0.08808053856094679, Scale 0: rho: 0.14990201592445374, Scale 0: flow: 0.009617072188605864, Scale 0: rec: 0.0009523910198671122, Scale 1: mask: 0.17511483132839203, Scale 1: rho: 0.3045249576369921, Scale 1: flow: 0.039959322474896905, Scale 1: rec: 0.00203923440615957, Scale 2: mask: 0.34636925160884857, Scale 2: rho: 0.5384404689073563, Scale 2: flow: 0.165412171681722, Scale 2: rec: 0.004255786382903655, Scale 3: mask: 0.7854970554510753, Scale 3: rho: 1.0648520400126775, Scale 3: flow: 1.4512868801752725, Scale 3: rec: 0.1738417277733485,  [Total Loss: 5.300145745533519]


 | Epoch (train): [1][120/150]
Scale 0: mask: 0.08746125375231108, Scale 0: rho: 0.14408535783489546, Scale 0: flow: 0.008924448878193896, Scale 0: rec: 0.0008845753657321135, Scale 1: mask: 0.17230877081553142, Scale 1: rho: 0.2892569522062937, Scale 1: flow: 0.0372435308371981, Scale 1: rec: 0.0018784834654070437, Scale 2: mask: 0.3470013827085495, Scale 2: rho: 0.570551319917043, Scale 2: flow: 0.15485325828194618, Scale 2: rec: 0.0039054577859739463, Scale 3: mask: 0.7296733518441518, Scale 3: rho: 1.1267704705397288, Scale 3: flow: 1.3301029483477274, Scale 3: rec: 0.17261093308528264,  [Total Loss: 5.177512495665966]


 | Epoch (train): [1][150/150]
Scale 0: mask: 0.08700911104679107, Scale 0: rho: 0.15214780668417613, Scale 0: flow: 0.009680526501809558, Scale 0: rec: 0.0008448832183300207, Scale 1: mask: 0.1761349787314733, Scale 1: rho: 0.29084585309028627, Scale 1: flow: 0.04023830766479174, Scale 1: rec: 0.0018592981932063898, Scale 2: mask: 0.3481283495823542, Scale 2: rho: 0.5935063610474268, Scale 2: flow: 0.16668172453840574, Scale 2: rec: 0.0039641060323144, Scale 3: mask: 0.7098354438940684, Scale 3: rho: 1.1169699360926946, Scale 3: flow: 1.3510996063550313, Scale 3: rec: 0.16462989275654158,  [Total Loss: 5.213576185429701]


 | Flow magnitude: Max 510.2137756347656, Min -511.804931640625, Mean 119.62408447265625
-->name: encoder0.0.encoder.conv.weight    -->weight tensor(-0.0011, device='cuda:0')
-->name: encoder0.0.encoder.conv.bias    -->weight tensor(-0.0334, device='cuda:0')
-->name: encoder0.0.encoder.batch_norm.weight    -->weight tensor(0.9999, device='cuda:0')
-->name: encoder0.0.encoder.batch_norm.bias    -->weight tensor(-0.0004, device='cuda:0')
-->name: encoder0.1.encoder.conv.weight    -->weight tensor(-0.0008, device='cuda:0')
-->name: encoder0.1.encoder.conv.bias    -->weight tensor(0.0019, device='cuda:0')
-->name: encoder0.1.encoder.batch_norm.weight    -->weight tensor(0.9961, device='cuda:0')
-->name: encoder0.1.encoder.batch_norm.bias    -->weight tensor(-0.0030, device='cuda:0')
-->name: encoder1.0.encoder.conv.weight    -->weight tensor(-0.0015, device='cuda:0')
-->name: encoder1.0.encoder.conv.bias    -->weight tensor(-0.0136, device='cuda:0')
-->name: encoder1.0.encoder.batch_norm.weight    -->weight tensor(1.0000, device='cuda:0')
-->name: encoder1.0.encoder.batch_norm.bias    -->weight tensor(-0.0007, device='cuda:0')
-->name: encoder1.1.encoder.conv.weight    -->weight tensor(0.0017, device='cuda:0')
-->name: encoder1.1.encoder.conv.bias    -->weight tensor(0.0116, device='cuda:0')
-->name: encoder1.1.encoder.batch_norm.weight    -->weight tensor(0.9981, device='cuda:0')
-->name: encoder1.1.encoder.batch_norm.bias    -->weight tensor(-0.0020, device='cuda:0')
-->name: encoder2.0.encoder.conv.weight    -->weight tensor(0.0009, device='cuda:0')
-->name: encoder2.0.encoder.conv.bias    -->weight tensor(0.0057, device='cuda:0')
-->name: encoder2.0.encoder.batch_norm.weight    -->weight tensor(0.9998, device='cuda:0')
-->name: encoder2.0.encoder.batch_norm.bias    -->weight tensor(-0.0010, device='cuda:0')
-->name: encoder2.1.encoder.conv.weight    -->weight tensor(0.0005, device='cuda:0')
-->name: encoder2.1.encoder.conv.bias    -->weight tensor(-0.0036, device='cuda:0')
-->name: encoder2.1.encoder.batch_norm.weight    -->weight tensor(0.9985, device='cuda:0')
-->name: encoder2.1.encoder.batch_norm.bias    -->weight tensor(-0.0016, device='cuda:0')
-->name: encoder3.0.encoder.conv.weight    -->weight tensor(0.0003, device='cuda:0')
-->name: encoder3.0.encoder.conv.bias    -->weight tensor(0.0027, device='cuda:0')
-->name: encoder3.0.encoder.batch_norm.weight    -->weight tensor(0.9998, device='cuda:0')
-->name: encoder3.0.encoder.batch_norm.bias    -->weight tensor(-0.0008, device='cuda:0')
-->name: encoder3.1.encoder.conv.weight    -->weight tensor(-1.7401e-05, device='cuda:0')
-->name: encoder3.1.encoder.conv.bias    -->weight tensor(-0.0003, device='cuda:0')
-->name: encoder3.1.encoder.batch_norm.weight    -->weight tensor(0.9991, device='cuda:0')
-->name: encoder3.1.encoder.batch_norm.bias    -->weight tensor(-0.0013, device='cuda:0')
-->name: encoder4.0.encoder.conv.weight    -->weight tensor(0.0002, device='cuda:0')
-->name: encoder4.0.encoder.conv.bias    -->weight tensor(0.0005, device='cuda:0')
-->name: encoder4.0.encoder.batch_norm.weight    -->weight tensor(0.9999, device='cuda:0')
-->name: encoder4.0.encoder.batch_norm.bias    -->weight tensor(-0.0007, device='cuda:0')
-->name: encoder4.1.encoder.conv.weight    -->weight tensor(9.1679e-05, device='cuda:0')
-->name: encoder4.1.encoder.conv.bias    -->weight tensor(-0.0002, device='cuda:0')
-->name: encoder4.1.encoder.batch_norm.weight    -->weight tensor(0.9998, device='cuda:0')
-->name: encoder4.1.encoder.batch_norm.bias    -->weight tensor(-0.0008, device='cuda:0')
-->name: encoder5.0.encoder.conv.weight    -->weight tensor(9.2115e-05, device='cuda:0')
-->name: encoder5.0.encoder.conv.bias    -->weight tensor(-0.0005, device='cuda:0')
-->name: encoder5.0.encoder.batch_norm.weight    -->weight tensor(0.9999, device='cuda:0')
-->name: encoder5.0.encoder.batch_norm.bias    -->weight tensor(-0.0004, device='cuda:0')
-->name: encoder5.1.encoder.conv.weight    -->weight tensor(3.1068e-05, device='cuda:0')
-->name: encoder5.1.encoder.conv.bias    -->weight tensor(-2.9188e-05, device='cuda:0')
-->name: encoder5.1.encoder.batch_norm.weight    -->weight tensor(0.9999, device='cuda:0')
-->name: encoder5.1.encoder.batch_norm.bias    -->weight tensor(-0.0006, device='cuda:0')
-->name: encoder6.0.encoder.conv.weight    -->weight tensor(4.0635e-05, device='cuda:0')
-->name: encoder6.0.encoder.conv.bias    -->weight tensor(0.0001, device='cuda:0')
-->name: encoder6.0.encoder.batch_norm.weight    -->weight tensor(0.9999, device='cuda:0')
-->name: encoder6.0.encoder.batch_norm.bias    -->weight tensor(-0.0004, device='cuda:0')
-->name: encoder6.1.encoder.conv.weight    -->weight tensor(1.3169e-05, device='cuda:0')
-->name: encoder6.1.encoder.conv.bias    -->weight tensor(0.0005, device='cuda:0')
-->name: encoder6.1.encoder.batch_norm.weight    -->weight tensor(0.9999, device='cuda:0')
-->name: encoder6.1.encoder.batch_norm.bias    -->weight tensor(-0.0003, device='cuda:0')


 | Epoch: [1] Losses summary: Scale 0: mask: 0.08728018512328466, Scale 0: rho: 0.1478811575472355, Scale 0: flow: 0.00928334377706051, Scale 0: rec: 0.0009068255987949669, Scale 1: mask: 0.1747265840570132, Scale 1: rho: 0.29055861761172613, Scale 1: flow: 0.03858575681845347, Scale 1: rec: 0.001931582543378075, Scale 2: mask: 0.34916972597440077, Scale 2: rho: 0.5699160153667131, Scale 2: flow: 0.15993000408013663, Scale 2: rec: 0.004036177539577086, Scale 3: mask: 0.725637921889623, Scale 3: rho: 1.1210679177443186, Scale 3: flow: 1.3903383787473043, Scale 3: rec: 0.17301391149560613,  [Total Loss: 5.244264105914627]

**** Epoch 1 saving checkpoint ****

Traceback (most recent call last):
  File "main.py", line 78, in <module>
    init()
  File "main.py", line 62, in init
    CheckPoint.save(args, model, trainer.optim_state, epoch+1)
  File "/home/ryan/Documents/ETOM-Net/checkpoint.py", line 39, in save
    torch.save(checkpoint, os.path.join(opt.save, 'checkpoint' + suffix + '.pt'))
  File "/home/ryan/.local/lib/python3.6/site-packages/torch/serialization.py", line 372, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/home/ryan/.local/lib/python3.6/site-packages/torch/serialization.py", line 476, in _save
    pickler.dump(obj)
AttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'
